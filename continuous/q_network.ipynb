{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import cPickle as pickle\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Albumin', 'Arterial_BE', 'Arterial_lactate', 'Arterial_pH', 'BUN', 'CO2_mEqL', 'Calcium', 'Chloride', 'Creatinine', 'DiaBP', 'FiO2_1', 'GCS', 'Glucose', 'HCO3', 'HR', 'Hb', 'INR', 'Ionised_Ca', 'Magnesium', 'MeanBP', 'PT', 'PTT', 'PaO2_FiO2', 'Platelets_count', 'Potassium', 'RR', 'SGOT', 'SGPT', 'SIRS', 'SOFA', 'Shock_Index', 'Sodium', 'SpO2', 'SysBP', 'Temp_C', 'Total_bili', 'WBC_count', 'Weight_kg', 'age', 'elixhauser', 'gender', 'mechvent', 'output_4hourly', 'output_total', 'paCO2', 'paO2', 're_admission']\n",
      "47\n"
     ]
    }
   ],
   "source": [
    "with open('../data/state_features.txt') as f:\n",
    "    state_features = f.read().split()\n",
    "print (state_features)\n",
    "print len(state_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/rl_train_data_final_cont.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1.1</th>\n",
       "      <th>bloc</th>\n",
       "      <th>icustayid</th>\n",
       "      <th>charttime</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>elixhauser</th>\n",
       "      <th>re_admission</th>\n",
       "      <th>died_in_hosp</th>\n",
       "      <th>mortality_90d</th>\n",
       "      <th>...</th>\n",
       "      <th>median_dose_vaso</th>\n",
       "      <th>max_dose_vaso</th>\n",
       "      <th>input_total_tev</th>\n",
       "      <th>input_4hourly_tev</th>\n",
       "      <th>output_total</th>\n",
       "      <th>output_4hourly</th>\n",
       "      <th>cumulated_balance_tev</th>\n",
       "      <th>vaso_input</th>\n",
       "      <th>iv_input</th>\n",
       "      <th>reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7245052800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.412568</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.797351</td>\n",
       "      <td>0.939195</td>\n",
       "      <td>0.589916</td>\n",
       "      <td>0.750908</td>\n",
       "      <td>0.554500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7245067200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.412568</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.831780</td>\n",
       "      <td>0.934543</td>\n",
       "      <td>0.674384</td>\n",
       "      <td>0.819589</td>\n",
       "      <td>0.580033</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>7245081600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.412568</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.833222</td>\n",
       "      <td>0.656575</td>\n",
       "      <td>0.765423</td>\n",
       "      <td>0.939329</td>\n",
       "      <td>0.555033</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7245096000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.412568</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.834033</td>\n",
       "      <td>0.603831</td>\n",
       "      <td>0.783597</td>\n",
       "      <td>0.847073</td>\n",
       "      <td>0.545700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>7245110400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.412568</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.834836</td>\n",
       "      <td>0.603831</td>\n",
       "      <td>0.794059</td>\n",
       "      <td>0.811583</td>\n",
       "      <td>0.539533</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1.1  bloc  icustayid   charttime  gender       age  elixhauser  \\\n",
       "0               0     1          3  7245052800     0.0  0.412568         0.0   \n",
       "1               1     2          3  7245067200     0.0  0.412568         0.0   \n",
       "2               2     3          3  7245081600     0.0  0.412568         0.0   \n",
       "3               3     4          3  7245096000     0.0  0.412568         0.0   \n",
       "4               4     5          3  7245110400     0.0  0.412568         0.0   \n",
       "\n",
       "   re_admission  died_in_hosp  mortality_90d   ...    median_dose_vaso  \\\n",
       "0           0.0             0              1   ...                 0.0   \n",
       "1           0.0             0              1   ...                 0.0   \n",
       "2           0.0             0              1   ...                 0.0   \n",
       "3           0.0             0              1   ...                 0.0   \n",
       "4           0.0             0              1   ...                 0.0   \n",
       "\n",
       "   max_dose_vaso  input_total_tev  input_4hourly_tev  output_total  \\\n",
       "0            0.0         0.797351           0.939195      0.589916   \n",
       "1            0.0         0.831780           0.934543      0.674384   \n",
       "2            0.0         0.833222           0.656575      0.765423   \n",
       "3            0.0         0.834033           0.603831      0.783597   \n",
       "4            0.0         0.834836           0.603831      0.794059   \n",
       "\n",
       "   output_4hourly  cumulated_balance_tev  vaso_input  iv_input  reward  \n",
       "0        0.750908               0.554500         0.0       4.0       0  \n",
       "1        0.819589               0.580033         0.0       4.0       0  \n",
       "2        0.939329               0.555033         0.0       2.0       0  \n",
       "3        0.847073               0.545700         0.0       2.0       0  \n",
       "4        0.811583               0.539533         0.0       2.0       0  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_df = pd.read_csv('../data/rl_val_data_final_cont.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('../data/rl_test_data_final_cont.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "REWARD_THRESHOLD = 15\n",
    "reg_lambda = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# PER important weights and params\n",
    "per_flag = True\n",
    "beta_start = 0.9\n",
    "df['prob'] = abs(df['reward'])\n",
    "temp = 1.0/df['prob']\n",
    "#temp[temp == float('Inf')] = 1.0\n",
    "df['imp_weight'] = pow((1.0/len(df) * temp), beta_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "hidden_1_size = 128\n",
    "hidden_2_size = 128\n",
    "#  Q-network uses Leaky ReLU activation\n",
    "class Qnetwork():\n",
    "    def __init__(self):\n",
    "        self.phase = tf.placeholder(tf.bool)\n",
    "\n",
    "        self.num_actions = 25\n",
    "\n",
    "        self.input_size = len(state_features)\n",
    "\n",
    "        self.state = tf.placeholder(tf.float32, shape=[None, self.input_size],name=\"input_state\")\n",
    "\n",
    "        self.fc_1 = tf.contrib.layers.fully_connected(self.state, hidden_1_size, activation_fn=None)\n",
    "        self.fc_1_bn = tf.contrib.layers.batch_norm(self.fc_1, center=True, scale=True, is_training=self.phase)\n",
    "        self.fc_1_ac = tf.maximum(self.fc_1_bn, self.fc_1_bn*0.5)\n",
    "        self.fc_2 = tf.contrib.layers.fully_connected(self.fc_1_ac, hidden_2_size, activation_fn=None)\n",
    "        self.fc_2_bn = tf.contrib.layers.batch_norm(self.fc_2, center=True, scale=True, is_training=self.phase)\n",
    "        self.fc_2_ac = tf.maximum(self.fc_2_bn, self.fc_2_bn*0.5)\n",
    "        \n",
    "        # advantage and value streams\n",
    "        self.streamA,self.streamV = tf.split(self.fc_2_ac,2,axis=1)\n",
    "        self.AW = tf.Variable(tf.random_normal([hidden_2_size//2,self.num_actions]))\n",
    "        self.VW = tf.Variable(tf.random_normal([hidden_2_size//2,1]))\n",
    "        self.Advantage = tf.matmul(self.streamA,self.AW)\n",
    "        self.Value = tf.matmul(self.streamV,self.VW)\n",
    "        \n",
    "        #Then combine them together to get our final Q-values.\n",
    "        self.q_output = self.Value + tf.subtract(self.Advantage,tf.reduce_mean(self.Advantage,axis=1,keep_dims=True))\n",
    "       \n",
    "        self.predict = tf.argmax(self.q_output,1, name='predict') # vector of length batch size\n",
    "        \n",
    "        #Below we obtain the loss by taking the sum of squares difference between the target and predicted Q values.\n",
    "        self.targetQ = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "        self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        self.actions_onehot = tf.one_hot(self.actions,self.num_actions,dtype=tf.float32)\n",
    "        \n",
    "        # Importance sampling weights for PER, used in network update         \n",
    "        self.imp_weights = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "        \n",
    "        # select the Q values for the actions that would be selected         \n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.q_output, self.actions_onehot), reduction_indices=1) # batch size x 1 vector\n",
    "        \n",
    "        \n",
    "        # regularisation penalises the network when it produces rewards that are above the\n",
    "        # reward threshold, to ensure reasonable Q-value predictions      \n",
    "        self.reg_vector = tf.maximum(tf.abs(self.Q)-REWARD_THRESHOLD,0)\n",
    "        self.reg_term = tf.reduce_sum(self.reg_vector)\n",
    "        \n",
    "        self.abs_error = tf.abs(self.targetQ - self.Q)\n",
    "        \n",
    "        self.td_error = tf.square(self.targetQ - self.Q)\n",
    "        \n",
    "        # below is the loss when we are not using PER\n",
    "        self.old_loss = tf.reduce_mean(self.td_error)\n",
    "        \n",
    "        # as in the paper, to get PER loss we weight the squared error by the importance weights\n",
    "        self.per_error = tf.multiply(self.td_error, self.imp_weights)\n",
    "\n",
    "        # total loss is a sum of PER loss and the regularisation term\n",
    "        if per_flag:\n",
    "            self.loss = tf.reduce_mean(self.per_error) + reg_lambda*self.reg_term\n",
    "        else:\n",
    "            self.loss = self.old_loss + reg_lambda*self.reg_term\n",
    "\n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        self.update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(self.update_ops):\n",
    "        # Ensures that we execute the update_ops before performing the model update, so batchnorm works\n",
    "            self.update_model = self.trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# function is needed to update parameters between main and target network\n",
    "# tf_vars are the trainable variables to update, and tau is the rate at which to update\n",
    "# returns tf ops corresponding to the updates\n",
    "def update_target_graph(tf_vars,tau):\n",
    "    total_vars = len(tf_vars)\n",
    "    op_holder = []\n",
    "    for idx,var in enumerate(tf_vars[0:int(total_vars/2)]):\n",
    "        op_holder.append(tf_vars[idx+int(total_vars/2)].assign((var.value()*tau) + ((1-tau)*tf_vars[idx+int(total_vars/2)].value())))\n",
    "    return op_holder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def update_target(op_holder,sess):\n",
    "    for op in op_holder:\n",
    "        sess.run(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# define an action mapping - how to get an id representing the action from the (iv,vaso) tuple\n",
    "action_map = {}\n",
    "count = 0\n",
    "for iv in range(5):\n",
    "    for vaso in range(5):\n",
    "        action_map[(iv,vaso)] = count\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# generates batches for the Q network - depending on train and eval_type, can select data from train/val/test sets.\n",
    "def process_batch(size, train=True, eval_type = None):\n",
    "    if not train:\n",
    "        if eval_type is None:\n",
    "            raise Exception('Provide eval_type to process_batch')\n",
    "        elif eval_type == 'train':\n",
    "            a = df.copy()\n",
    "        elif eval_type == 'val':\n",
    "            a = val_df.copy()\n",
    "        elif eval_type == 'test':\n",
    "            a = test_df.copy()\n",
    "        else:\n",
    "            raise Exception('Unknown eval_type')\n",
    "    else:\n",
    "        if per_flag:\n",
    "            # uses prioritised exp replay\n",
    "            a = df.sample(n=size, weights=df['prob'])\n",
    "        else:\n",
    "            a = df.sample(n=size)\n",
    "    states = None\n",
    "    actions = None\n",
    "    rewards = None\n",
    "    next_states = None\n",
    "    done_flags = None\n",
    "    for i in a.index:\n",
    "        cur_state = a.ix[i,state_features]\n",
    "        iv = int(a.ix[i, 'iv_input'])\n",
    "        vaso = int(a.ix[i, 'vaso_input'])\n",
    "        action = action_map[iv,vaso]\n",
    "        reward = a.ix[i,'reward']\n",
    "\n",
    "        if i != df.index[-1]:\n",
    "            # if not terminal step in trajectory             \n",
    "            if df.ix[i, 'icustayid'] == df.ix[i+1, 'icustayid']:\n",
    "                next_state = df.ix[i + 1, state_features]\n",
    "                done = 0\n",
    "            else:\n",
    "                # trajectory is finished\n",
    "                next_state = np.zeros(len(cur_state))\n",
    "                done = 1\n",
    "        else:\n",
    "            # last entry in df is the final state of that trajectory\n",
    "            next_state = np.zeros(len(cur_state))\n",
    "            done = 1\n",
    "\n",
    "        if states is None:\n",
    "            states = copy.deepcopy(cur_state)\n",
    "        else:\n",
    "            states = np.vstack((states,cur_state))\n",
    "\n",
    "        if actions is None:\n",
    "            actions = [action]\n",
    "        else:\n",
    "            actions = np.vstack((actions,action))\n",
    "\n",
    "        if rewards is None:\n",
    "            rewards = [reward]\n",
    "        else:\n",
    "            rewards = np.vstack((rewards,reward))\n",
    "\n",
    "        if next_states is None:\n",
    "            next_states = copy.deepcopy(next_state)\n",
    "        else:\n",
    "            next_states = np.vstack((next_states,next_state))\n",
    "\n",
    "        if done_flags is None:\n",
    "            done_flags = [done]\n",
    "        else:\n",
    "            done_flags = np.vstack((done_flags,done))\n",
    "    \n",
    "    return (states, np.squeeze(actions), np.squeeze(rewards), next_states, np.squeeze(done_flags), a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#  Used to run diagnostics on the train set\n",
    "phys_q_train = []\n",
    "agent_q_train = []\n",
    "phys_actions_tr = []\n",
    "agent_actions_tr = []\n",
    "def train_set_performance():\n",
    "    count = 0\n",
    "    global phys_q_train\n",
    "    global agent_q_train\n",
    "    global phys_actions\n",
    "    global agent_actions\n",
    "    phys_q_train = []\n",
    "    agent_q_train = []\n",
    "    phys_actions_tr = []\n",
    "    agent_actions_tr = []\n",
    "    for r in df.index:\n",
    "        cur_state = [df.ix[r,state_features]]\n",
    "        iv = int(df.ix[r, 'iv_input'])\n",
    "        vaso = int(df.ix[r, 'vaso_input'])\n",
    "        action = action_map[iv,vaso]\n",
    "        output_q = np.squeeze(sess.run(mainQN.q_output, feed_dict = {mainQN.state : cur_state, mainQN.phase : False}))\n",
    "        phys_q_train.append(output_q[action])\n",
    "        agent_q_train.append(max(output_q))\n",
    "        agent_actions_tr.append(np.argmax(output_q))\n",
    "        phys_actions_tr.append(action)\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_eval(eval_type)\n",
    "    states,actions,rewards,next_states, done_flags, _ = process_batch(size=None,train=False,eval_type=eval_type)\n",
    "\n",
    "    # firstly get the chosen actions at the next timestep\n",
    "    actions_from_q1 = sess.run(mainQN.predict,feed_dict={mainQN.state:next_states, mainQN.phase : 0})\n",
    "\n",
    "    # Q values for the next timestep from target network, as part of the Double DQN update\n",
    "    Q2 = sess.run(targetQN.q_output,feed_dict={targetQN.state:next_states, targetQN.phase : 0})\n",
    "\n",
    "    # handles the case when a trajectory is finished\n",
    "    end_multiplier = 1 - done_flags\n",
    "\n",
    "    # target Q value using Q values from target, and actions from main\n",
    "    double_q_value = Q2[range(batch_size),actions_from_q1]\n",
    "\n",
    "    # definition of target Q\n",
    "    targetQ = rewards + (gamma*double_q_value * end_multiplier)\n",
    "\n",
    "    # get the output q's, actions, and loss\n",
    "    q_output,actions_taken, loss = sess.run([mainQN.q_output,mainQN.predict, mainQN.abs_error], \\\n",
    "        feed_dict={mainQN.state:states,\n",
    "                   mainQN.targetQ:targetQ, \n",
    "                   mainQN.actions:actions,\n",
    "                   mainQN.phase:False})\n",
    "    \n",
    "    # return the relevant q values and actions\n",
    "    phys_q = q_output[range(len(q_output)), actions]\n",
    "    agent_q = q_output[range(len(q_output)), actions_taken]\n",
    "    error = np.mean(abs_error)\n",
    "    return phys_q, actions, agent_q, actions_taken, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # Don't use all GPUs \n",
    "config.allow_soft_placement = True  # Enable manual control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to load model...\n",
      "Model restored\n",
      "PER and Importance weights restored\n",
      "Init done\n",
      "Saved Model, step is 1000\n",
      "('Average loss is ', 0.29190072822570801)\n",
      "Saving PER and importance weights\n",
      "physactions  [10 10  0 20  0  5  0 10 15 15  0  0 20  5  0  0 14 15 10  0 24  0  5  5  0\n",
      " 10  5  0  5 18]\n",
      " chosen actions  [ 2 15 23  9  9  5 19  2 16 15  5  0 23  0  7 21  2 14 14  0 21 20 18  1 23\n",
      "  0  8 24  0 10]\n",
      "0.0\n",
      "5.82176\n",
      "9.60063\n"
     ]
    }
   ],
   "source": [
    "# The main training loop is here\n",
    "per_alpha = 0.6 # PER hyperparameter\n",
    "per_epsilon = 0.01 # PER hyperparameter\n",
    "batch_size = 30\n",
    "gamma = 1 # discount factor \n",
    "num_steps = 200000 # How many steps to train for\n",
    "load_model = True #Whether to load a saved model.\n",
    "save_dir = \"./dqn_normal/'\n",
    "save_path = \"./dqn_normal/ckpt\"#The path to save our model to.\n",
    "tau = 0.001 #Rate to update target network toward primary network\n",
    "tf.reset_default_graph()\n",
    "mainQN = Qnetwork()\n",
    "targetQN = Qnetwork()\n",
    "av_q_list = []\n",
    "\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "trainables = tf.trainable_variables()\n",
    "\n",
    "target_ops = update_target_graph(trainables,tau)\n",
    "\n",
    "#Make a path for our model to be saved in.\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    if load_model == True:\n",
    "        print('Trying to load model...')\n",
    "        try:\n",
    "            restorer = tf.train.import_meta_graph(save_path + '.meta')\n",
    "            restorer.restore(sess, tf.train.latest_checkpoint(save_dir))\n",
    "            print \"Model restored\"\n",
    "        except IOError:\n",
    "            print \"No previous model found, running default init\"\n",
    "            sess.run(init)\n",
    "        try:\n",
    "            per_weights = pickle.load(open( save_dir + \"per_weights.p\", \"rb\" ))\n",
    "            imp_weights = pickle.load(open( save_dir + \"imp_weights.p\", \"rb\" ))\n",
    "            \n",
    "            # the PER weights, governing probability of sampling, and importance sampling\n",
    "            # weights for use in the gradient descent updates\n",
    "            df['prob'] = per_weights\n",
    "            df['imp_weight'] = imp_weights\n",
    "            print \"PER and Importance weights restored\"\n",
    "        except IOError:\n",
    "            print(\"No PER weights found - default being used for PER and importance sampling\")\n",
    "    else:\n",
    "        print(\"Running default init\")\n",
    "        sess.run(init)\n",
    "    print(\"Init done\")\n",
    "    for i in range(num_steps):\n",
    "        net_loss = 0.0\n",
    "        net_q = 0.0\n",
    "        states,actions,rewards,next_states, done_flags, sampled_df = process_batch(batch_size)\n",
    "        \n",
    "        # firstly get the chosen actions at the next timestep\n",
    "        actions_from_q1 = sess.run(mainQN.predict,feed_dict={mainQN.state:next_states, mainQN.phase : 1})\n",
    "        \n",
    "        # actions chosen now, as a check\n",
    "        cur_act = sess.run(mainQN.predict,feed_dict={mainQN.state:states, mainQN.phase : 1})\n",
    "\n",
    "        # Q values for the next timestep from target network, as part of the Double DQN update\n",
    "        Q2 = sess.run(targetQN.q_output,feed_dict={targetQN.state:next_states, targetQN.phase : 1})\n",
    "\n",
    "        # handles the case when a trajectory is finished\n",
    "        end_multiplier = 1 - done_flags\n",
    "    \n",
    "        # target Q value using Q values from target, and actions from main\n",
    "        double_q_value = Q2[range(batch_size),actions_from_q1]\n",
    "        \n",
    "        # empirical hack to make the Q values never exceed the threshold - helps learning\n",
    "        double_q_value[double_q_value > REWARD_THRESHOLD] = REWARD_THRESHOLD\n",
    "        double_q_value[double_q_value < -REWARD_THRESHOLD] = -REWARD_THRESHOLD\n",
    "        \n",
    "        # definition of target Q\n",
    "        targetQ = rewards + (gamma*double_q_value * end_multiplier)\n",
    "\n",
    "        # Calculate the importance sampling weights for PER\n",
    "        imp_sampling_weights = np.array(sampled_df['imp_weight'] / float(max(df['imp_weight'])))\n",
    "        imp_sampling_weights[np.isnan(imp_sampling_weights)] = 1\n",
    "        imp_sampling_weights[imp_sampling_weights <= 0.001] = 0.001\n",
    "\n",
    "        # Train with the batch\n",
    "        _,loss, error = sess.run([mainQN.update_model,mainQN.loss, mainQN.abs_error], \\\n",
    "            feed_dict={mainQN.state:states,\n",
    "                       mainQN.targetQ:targetQ, \n",
    "                       mainQN.actions:actions,\n",
    "                       mainQN.phase:True,\n",
    "                       mainQN.imp_weights:imp_sampling_weights})\n",
    "\n",
    "        # Update target towards main network\n",
    "        update_target(target_ops,sess)\n",
    "        \n",
    "        net_loss += sum(error)\n",
    "        net_q += np.mean(targetQ)\n",
    "        \n",
    "        # Set the selection weight/prob to the abs prediction error and update the importance sampling weight\n",
    "        new_weights = pow((error + per_epsilon), per_alpha)\n",
    "        df.ix[df.index.isin(sampled_df.index), 'prob'] = new_weights\n",
    "        temp = 1.0/new_weights\n",
    "        df.ix[df.index.isin(sampled_df.index), 'imp_weight'] = pow(((1.0/len(df)) * temp), beta_start)\n",
    "        \n",
    "        if i % 1000 == 0 and i > 0:\n",
    "            saver.save(sess,save_path)\n",
    "            print(\"Saved Model, step is \" + str(i))\n",
    "            \n",
    "            av_loss = net_loss/1000.0\n",
    "            print(\"Average loss is \", av_loss)\n",
    "            net_loss = 0.0\n",
    "                        \n",
    "            print (\"Saving PER and importance weights\")\n",
    "            with open(save_dir + 'per_weights.p', 'wb') as f:\n",
    "                pickle.dump(df['prob'], f)\n",
    "            with open(save_dir + 'imp_weights.p', 'wb') as f:\n",
    "                pickle.dump(df['imp_weight'], f)\n",
    "        \n",
    "        if (i % 1000==0) and i > 0:\n",
    "            print \"physactions \", actions\n",
    "            print \" chosen actions \", cur_act\n",
    "            if i >= 1000:\n",
    "                # run an evaluation on the validation set\n",
    "                phys_q, phys_actions, agent_q, agent_actions, mean_abs_error = do_eval(eval_type = 'val')        \n",
    "                print mean_abs_error\n",
    "                print np.mean(phys_q)\n",
    "                print np.mean(agent_q)\n",
    "                break\n",
    "#     saver.save(sess,save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2     8528\n",
       "0     6446\n",
       "8     5716\n",
       "16    3947\n",
       "15    3362\n",
       "10    3040\n",
       "20    3034\n",
       "23    2598\n",
       "5     2555\n",
       "18    2180\n",
       "21    1900\n",
       "12    1205\n",
       "13    1111\n",
       "1      505\n",
       "14     445\n",
       "9      414\n",
       "22     358\n",
       "7      323\n",
       "11     278\n",
       "17     210\n",
       "3      177\n",
       "24     152\n",
       "6      145\n",
       "4       95\n",
       "19      72\n",
       "dtype: int64"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(agent_actions).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f6cefd68750>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAGQtJREFUeJzt3X+MXOV97/H3pzgEF/diOyQjy/a9dhsrFcEKhRVQJarG\n8Y0xpKpdKbGIrGaNXLl/OL3JlaViKkVO+XHlXEFp4Dbo7o3dmtTNxqWhtggttRxGaf+AgIGL+VGu\nN2CCV8ZuWeN0A0m1yff+Mc/CYT3jnZmdH+t5Pi/J2nO+85xnnu+c9X7nPHPmHEUEZmaWp1/q9QDM\nzKx3XATMzDLmImBmljEXATOzjLkImJllzEXAzCxjLgJmZhlzETAzy5iLgJlZxub0egDncumll8ay\nZcta3v4nP/kJF198cfsGdB5x7nnmDnnnn3Pu8G7+hw8f/reI+GAj28zqIrBs2TKefPLJlrevVCqU\ny+X2Deg84tzLvR5Gz+Scf865w7v5S3q10W08HWRmljEXATOzjLkImJllzEXAzCxjLgJmZhlzETAz\ny5iLgJlZxlwEzMwy5iJgZpaxWf2N4dlu2fbv1owf2/npLo/EzKw1PhIwM8uYi4CZWcZcBMzMMuYi\nYGaWMRcBM7OMuQiYmWXMRcDMLGMuAmZmGWuoCEj675Kel/ScpG9JukjSckmPSxqR9G1JF6a270/r\nI+nxZYV+bknxlyRd15mUzMysUdMWAUmLgf8GDETE5cAFwI3AV4G7I+LDwGlgc9pkM3A6xe9O7ZB0\nWdruo8Ba4OuSLmhvOmZm1oxGp4PmAHMlzQF+GTgBfBJ4ID2+B1ifltelddLjqyUpxYcj4mcR8Qow\nAlw98xTMzKxV0xaBiBgF7gR+RPWP/xngMPBmREykZseBxWl5MfBa2nYitf9AMV5jGzMz64FpLyAn\naQHVd/HLgTeBv6E6ndMRkrYAWwBKpRKVSqXlvsbHx2e0/XS2rZyoGe/kczaq07nPZjnnDnnnn3Pu\n0Fr+jVxF9L8Cr0TEvwJI+g7wcWC+pDnp3f4SYDS1HwWWAsfT9NElwBuF+KTiNu+IiCFgCGBgYCDK\n5XJTCRVVKhVmsv10NtW7iujGzj1nozqd+2yWc+6Qd/455w6t5d/IZwI/Aq6V9Mtpbn818ALwKPCZ\n1GYQ2J+WD6R10uPfi4hI8RvT2UPLgRXAD5oarZmZtdW0RwIR8bikB4CngAngaarv1L8LDEu6PcV2\npU12Ad+UNAKMUT0jiIh4XtI+qgVkAtgaET9vcz5mZtaEhm4qExE7gB1Twi9T4+yeiPgp8Nk6/dwB\n3NHkGM3MrEP8jWEzs4y5CJiZZcxFwMwsYy4CZmYZcxEwM8uYi4CZWcZcBMzMMuYiYGaWMRcBM7OM\nuQiYmWXMRcDMLGMuAmZmGXMRMDPLmIuAmVnGXATMzDI2bRGQ9BFJzxT+/VjSlyQtlHRQ0tH0c0Fq\nL0n3SBqR9KykKwt9Dab2RyUN1n9WMzPrhmmLQES8FBFXRMQVwFXAW8CDwHbgUESsAA6ldYDrqd46\ncgXVG8bfByBpIdUb01xD9WY0OyYLh5mZ9Uaz00GrgR9GxKvAOmBPiu8B1qfldcD9UfUY1RvSLwKu\nAw5GxFhEnAYOAmtnnIGZmbWs2SJwI/CttFyKiBNp+XWglJYXA68VtjmeYvXiZmbWIw3dYxhA0oXA\n7wC3TH0sIkJStGNAkrZQnUaiVCpRqVRa7mt8fHxG209n28qJmvFOPmejOp37bJZz7pB3/jnnDq3l\n33ARoDrX/1REnEzrJyUtiogTabrnVIqPAksL2y1JsVGgPCV+1mgjYggYAhgYGIhyuTy1ScMqlQoz\n2X46m7Z/t2b82MbOPWejOp37bJZz7pB3/jnnDq3l38x00Od4dyoI4AAweYbPILC/EP98OkvoWuBM\nmjZ6BFgjaUH6QHhNipmZWY80dCQg6WLgU8AfFMI7gX2SNgOvAhtS/GHgBmCE6plENwFExJik24An\nUrtbI2JsxhmYmVnLGioCEfET4ANTYm9QPVtoatsAttbpZzewu/lhmplZJ/gbw2ZmGXMRMDPLmIuA\nmVnGXATMzDLmImBmljEXATOzjLkImJllzEXAzCxjzVw76LxzZPRMzev7HNv56R6Mxsxs9vGRgJlZ\nxlwEzMwy5iJgZpYxFwEzs4y5CJiZZcxFwMwsYw0VAUnzJT0g6V8kvSjpNyUtlHRQ0tH0c0FqK0n3\nSBqR9KykKwv9DKb2RyUN1n9GMzPrhkaPBL4G/ENE/DrwMeBFYDtwKCJWAIfSOlTvRbwi/dsC3Acg\naSGwA7gGuBrYMVk4zMysN6YtApIuAX4L2AUQEf8REW8C64A9qdkeYH1aXgfcH1WPAfPTjeivAw5G\nxFhEnAYOAmvbmo2ZmTWlkSOB5cC/An8h6WlJ30j3HC6lG8gDvA6U0vJi4LXC9sdTrF7czMx6pJHL\nRswBrgT+MCIel/Q13p36Aar3FZYU7RiQpC1Up5EolUpUKpWW+yrNhW0rJ86Kz6TPolp9t7P/mRgf\nH58V4+iFnHOHvPPPOXdoLf9GisBx4HhEPJ7WH6BaBE5KWhQRJ9J0z6n0+CiwtLD9khQbBcpT4meN\nNiKGgCGAgYGBKJfLU5s07N69+7nryNkpHtvYep9Fta5L1M7+Z6JSqTCT1+58lnPukHf+OecOreU/\n7XRQRLwOvCbpIym0GngBOABMnuEzCOxPyweAz6ezhK4FzqRpo0eANZIWpA+E16SYmZn1SKNXEf1D\nYK+kC4GXgZuoFpB9kjYDrwIbUtuHgRuAEeCt1JaIGJN0G/BEandrRIy1JQszM2tJQ0UgIp4BBmo8\ntLpG2wC21ulnN7C7mQGamVnn+BvDZmYZcxEwM8tYX99ZzHpvWb0zqHx3N7NZwUcCZmYZcxEwM8uY\ni4CZWcZcBMzMMuYiYGaWMRcBM7OMuQiYmWXMRcDMLGMuAmZmGXMRMDPLmIuAmVnGXATMzDLmImBm\nlrGGioCkY5KOSHpG0pMptlDSQUlH088FKS5J90gakfSspCsL/Qym9kclDdZ7PjMz645mjgRWRcQV\nETF5h7HtwKGIWAEcSusA1wMr0r8twH1QLRrADuAa4Gpgx2ThMDOz3pjJdNA6YE9a3gOsL8Tvj6rH\ngPmSFgHXAQcjYiwiTgMHgbUzeH4zM5shVW8JPE0j6RXgNBDA/46IIUlvRsT89LiA0xExX9JDwM6I\n+Of02CHgZqAMXBQRt6f4l4G3I+LOKc+1heoRBKVS6arh4eGWkzs1doaTb58dX7n4kpb7LDoyeqZm\nvF39z8T4+Djz5s3r9TB68hrNltx7Jef8c84d3s1/1apVhwuzNufU6J3FPhERo5I+BByU9C/FByMi\nJE1fTRoQEUPAEMDAwECUy+WW+7p3737uOnJ2isc2tt5n0aZ6d81qU/8zUalUmMlr1y69eI1mS+69\nknP+OecOreXf0HRQRIymn6eAB6nO6Z9M0zykn6dS81FgaWHzJSlWL25mZj0ybRGQdLGkX5lcBtYA\nzwEHgMkzfAaB/Wn5APD5dJbQtcCZiDgBPAKskbQgfSC8JsXMzKxHGpkOKgEPVqf9mQP8dUT8g6Qn\ngH2SNgOvAhtS+4eBG4AR4C3gJoCIGJN0G/BEandrRIy1LRMzM2vatEUgIl4GPlYj/gawukY8gK11\n+toN7G5+mGZm1gn+xrCZWcZcBMzMMuYiYGaWMRcBM7OMuQiYmWXMRcDMLGMuAmZmGXMRMDPLmIuA\nmVnGXATMzDLmImBmljEXATOzjLkImJllzEXAzCxjLgJmZhlruAhIukDS0+lG8khaLulxSSOSvi3p\nwhR/f1ofSY8vK/RxS4q/JOm6didjZmbNaeZI4IvAi4X1rwJ3R8SHgdPA5hTfDJxO8btTOyRdBtwI\nfBRYC3xd0gUzG76Zmc1EQ0VA0hLg08A30rqATwIPpCZ7gPVpeV1aJz2+OrVfBwxHxM8i4hWqt5+8\nuh1JmJlZaxo9Evgz4I+AX6T1DwBvRsREWj8OLE7Li4HXANLjZ1L7d+I1tjEzsx6Y9h7Dkn4bOBUR\nhyWVOz0gSVuALQClUolKpdJyX6W5sG3lxFnxmfRZVKvvdvY/E+Pj47NiHL14jWZL7r2Sc/455w6t\n5T9tEQA+DvyOpBuAi4D/BHwNmC9pTnq3vwQYTe1HgaXAcUlzgEuANwrxScVt3hERQ8AQwMDAQJTL\n5aYSKrp3737uOnJ2isc2tt5n0abt360Zb1f/M1GpVJjJa9cuvXiNZkvuvdJo/svq7BuAYzs/3cYR\ndY/3ffP5TzsdFBG3RMSSiFhG9YPd70XERuBR4DOp2SCwPy0fSOukx78XEZHiN6azh5YDK4AfNDVa\nMzNrq0aOBOq5GRiWdDvwNLArxXcB35Q0AoxRLRxExPOS9gEvABPA1oj4+Qye38zMZqipIhARFaCS\nll+mxtk9EfFT4LN1tr8DuKPZQZqZWWf4G8NmZhlzETAzy5iLgJlZxlwEzMwy5iJgZpYxFwEzs4y5\nCJiZZcxFwMwsYy4CZmYZcxEwM8vYTK4dZGazyORVQbetnHjP1VvP1yuCWnf4SMDMLGMuAmZmGXMR\nMDPLmIuAmVnGpi0Cki6S9ANJ/1fS85L+JMWXS3pc0oikb0u6MMXfn9ZH0uPLCn3dkuIvSbquU0mZ\nmVljGjkS+BnwyYj4GHAFsFbStcBXgbsj4sPAaWBzar8ZOJ3id6d2SLqM6l3GPgqsBb4u6YJ2JmNm\nZs1p5B7DERHjafV96V8AnwQeSPE9wPq0vC6tkx5fLUkpPhwRP4uIV4ARatyZzMzMuqeh7wmkd+yH\ngQ8Dfw78EHgzIiZSk+PA4rS8GHgNICImJJ0BPpDijxW6LW5j1pJlhfPhJ21bOUG5+0OxHpi6/ye/\nI+HvRjROEdF4Y2k+8CDwZeAv05QPkpYCfx8Rl0t6DlgbEcfTYz8ErgG+AjwWEX+V4rvSNg9MeY4t\nwBaAUql01fDwcMvJnRo7w8m3z46vXHxJy30WHRk9UzPerv5nYnx8nHnz5vV6GB1/jWr1X5oLH1rY\n+33QbZOvRWku7/m9r/da19s359pmtpmaw2Tu58v4223y//2qVasOR8RAI9s0e6P5NyU9CvwmMF/S\nnHQ0sAQYTc1GgaXAcUlzgEuANwrxScVtis8xBAwBDAwMRLlcbmaI73Hv3v3cdeTsFI9tbL3Pok01\n3oW2s/+ZqFQqzOS1a5dOv0a1+t+2coINsyD3bttU+MZw8fe+3mtdb9+ca5vZZmoOk7mfL+Nvt1b+\n3zdydtAH0xEAkuYCnwJeBB4FPpOaDQL70/KBtE56/HtRPdw4ANyYzh5aDqwAftDUaM3MrK0aORJY\nBOxJnwv8ErAvIh6S9AIwLOl24GlgV2q/C/impBFgjOoZQUTE85L2AS8AE8DWiPh5e9MxM7NmTFsE\nIuJZ4DdqxF+mxtk9EfFT4LN1+roDuKP5YZqZWSf4G8NmZhlzETAzy5iLgJlZxlwEzMwy5iJgZpYx\n317SzKyNal3KBGbvbT5dBPpI8ZfP95k1s0Z4OsjMLGMuAmZmGXMRMDPLmIuAmVnGXATMzDLmImBm\nljEXATOzjLkImJllzF8WM7OOOd++PZujRm4vuVTSo5JekPS8pC+m+EJJByUdTT8XpLgk3SNpRNKz\nkq4s9DWY2h+VNFjvOc3MrDsamQ6aALZFxGXAtcBWSZcB24FDEbECOJTWAa6nev/gFcAW4D6oFg1g\nB3AN1TuS7ZgsHGZm1hvTFoGIOBERT6Xlf6d6k/nFwDpgT2q2B1ifltcB90fVY8B8SYuA64CDETEW\nEaeBg8DatmZjZmZNUUQ03lhaBnwfuBz4UUTMT3EBpyNivqSHgJ0R8c/psUPAzUAZuCgibk/xLwNv\nR8SdU55jC9UjCEql0lXDw8MtJ3dq7Awn3z47vnLxJS33WXRk9EzNeLv6b1ZxPKW5vCf32TCmok7u\ng9Jc+NDC3uTbS5OvRaP7vt6+Odc2rY6pW/1P5t6r33fo7d+F8fFx5s2bx6pVqw5HxEAj2zT8wbCk\necDfAl+KiB9X/+5XRURIaryanENEDAFDAAMDA1Eul1vu6969+7nryNkpHtvYep9Fm+p96NWm/pu1\nacpVRIu5z4YxFXVyH2xbOcGGGfzenK8mX4tG9329fXOubVodU7f6n8y9V7/v0Nu/C5VKhWb/ZjZ0\niqik91EtAHsj4jspfDJN85B+nkrxUWBpYfMlKVYvbmZmPTLtkUCa6tkFvBgRf1p46AAwCOxMP/cX\n4l+QNEz1Q+AzEXFC0iPA/yh8GLwGuKU9aZiZTc+nrJ6tkemgjwO/BxyR9EyK/THVP/77JG0GXgU2\npMceBm4ARoC3gJsAImJM0m3AE6ndrREx1pYszMysJdMWgfQBr+o8vLpG+wC21ulrN7C7mQGamVnn\n+LIRZmYZcxEwM8uYi4CZWcZcBMzMMuYiYGaWMRcBM7OMuQiYmWXMRcDMLGMuAmZmGXMRMDPLmIuA\nmVnGXATMzDLmImBmljEXATOzjDV8e0kzey/foMT6wbRHApJ2Szol6blCbKGkg5KOpp8LUlyS7pE0\nIulZSVcWthlM7Y9KGuxMOmZm1oxGjgT+EvhfwP2F2HbgUETslLQ9rd8MXA+sSP+uAe4DrpG0ENgB\nDAABHJZ0ICJOtysRs9nORw42G017JBAR3wem3gZyHbAnLe8B1hfi90fVY8D8dBP664CDETGW/vAf\nBNa2IwEzM2tdqx8MlyLiRFp+HSil5cXAa4V2x1OsXtzMzHpI1VsCT9NIWgY8FBGXp/U3I2J+4fHT\nEbFA0kPAznRfYiQdojpNVAYuiojbU/zLwNsRcWeN59oCbAEolUpXDQ8Pt5zcqbEznHz77PjKxZe0\n3GfRkdEzNePt6r9ZxfGU5vKe3GfDmIo6uQ9Kc+FDCzufb7O5deu1aHTf1xtPJ8bUrf4nc+/VPujW\nc9QzPj7OvHnzWLVq1eGIGGhkm1bPDjopaVFEnEjTPadSfBRYWmi3JMVGqRaCYrxSq+OIGAKGAAYG\nBqJcLtdq1pB79+7nriNnp3hsY+t9Fm2qN8fbpv6bVRzPtpUT78l9NoypqJP7YNvKCTbM4PdmJs8N\n9XPr1mvR6L6vN55OjKlb/U/m3qt90K3nqKdSqdDs38xWp4MOAJNn+AwC+wvxz6ezhK4FzqRpo0eA\nNZIWpDOJ1qSYmZn10LRHApK+RfVd/KWSjlM9y2cnsE/SZuBVYENq/jBwAzACvAXcBBARY5JuA55I\n7W6NiKkfNpuZWZdNWwQi4nN1Hlpdo20AW+v0sxvY3dTozMyso3zZCDOzjLkImJllzEXAzCxjLgJm\nZhlzETAzy5iLgJlZxlwEzMwy5pvKFPhSv2aWGxcBs1nKb0qsG1wEbFbxHz6z7nIRMDOro96bEuif\nNyYuAtYUv1M36y8uApaVHN7ZmTXDp4iamWXMRwJm1jBPB/afrhcBSWuBrwEXAN+IiJ3dHkOvzMb/\nQLNxTGY56fX/wa4WAUkXAH8OfAo4Djwh6UBEvNDNcZwvev3LYWb9r9tHAlcDIxHxMoCkYWAd4CJg\nZn7j0wPd/mB4MfBaYf14ipmZWQ+oelvgLj2Z9BlgbUT8flr/PeCaiPhCoc0WYEta/Qjw0gye8lLg\n32aw/fnMuecr5/xzzh3ezf+/RMQHG9mg29NBo8DSwvqSFHtHRAwBQ+14MklPRsRAO/o63zj3PHOH\nvPPPOXdoLf9uTwc9AayQtFzShcCNwIEuj8HMzJKuHglExISkLwCPUD1FdHdEPN/NMZiZ2bu6/j2B\niHgYeLhLT9eWaaXzlHPPV87555w7tJB/Vz8YNjOz2cXXDjIzy1hfFgFJayW9JGlE0vZej6fbJB2T\ndETSM5Ke7PV4OknSbkmnJD1XiC2UdFDS0fRzQS/H2El18v+KpNG0/5+RdEMvx9gpkpZKelTSC5Ke\nl/TFFO/7/X+O3Jve9303HZQuTfH/KFyaAvhcTpemkHQMGIiIvj9fWtJvAePA/RFxeYr9T2AsInam\nNwELIuLmXo6zU+rk/xVgPCLu7OXYOk3SImBRRDwl6VeAw8B6YBN9vv/PkfsGmtz3/Xgk8M6lKSLi\nP4DJS1NYH4qI7wNjU8LrgD1peQ/V/xx9qU7+WYiIExHxVFr+d+BFqlcg6Pv9f47cm9aPRcCXpoAA\n/lHS4fQN7NyUIuJEWn4dKPVyMD3yBUnPpumivpsOmUrSMuA3gMfJbP9PyR2a3Pf9WAQMPhERVwLX\nA1vTlEGWojrf2V9zntO7D/g14ArgBHBXb4fTWZLmAX8LfCkiflx8rN/3f43cm973/VgEpr00Rb+L\niNH08xTwINUpspycTHOmk3Onp3o8nq6KiJMR8fOI+AXwf+jj/S/pfVT/CO6NiO+kcBb7v1burez7\nfiwCWV+aQtLF6YMiJF0MrAGeO/dWfecAMJiWB4H9PRxL103+AUx+lz7d/5IE7AJejIg/LTzU9/u/\nXu6t7Pu+OzsIIJ0W9We8e2mKO3o8pK6R9KtU3/1D9Rvhf93P+Uv6FlCmevXEk8AO4O+AfcB/Bl4F\nNkREX354Wif/MtXpgACOAX9QmCPvG5I+AfwTcAT4RQr/MdW58b7e/+fI/XM0ue/7sgiYmVlj+nE6\nyMzMGuQiYGaWMRcBM7OMuQiYmWXMRcDMLGMuAmZmGXMRMDPLmIuAmVnG/j+YGSAo9nZRUQAAAABJ\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6ce57666d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "% matplotlib inline\n",
    "pd.Series(agent_actions).hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the chosen actions for the train, val, and test set when training is complete.\n",
    "_, _, agent_q_train, agent_actions_train, _ = do_eval(eval_type = 'train')        \n",
    "_, _, agent_q_val, agent_actions_val, _ = do_eval(eval_type = 'val')        \n",
    "_, _, agent_q_test, agent_actions_test, _ = do_eval(eval_type = 'test')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save everything for later - they're used in policy evaluation and when generating plots\n",
    "with open(save_dir + 'dqn_normal_actions_train.p', 'wb') as f:\n",
    "    pickle.dump(agent_actions_train, f)\n",
    "with open(save_dir + 'dqn_normal_actions_val.p', 'wb') as f:\n",
    "    pickle.dump(agent_actions_val, f)\n",
    "with open(save_dir + 'dqn_normal_actions_test.p', 'wb') as f:\n",
    "    pickle.dump(agent_actions_test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(save_dir + 'dqn_normal_q_train.p', 'wb') as f:\n",
    "    pickle.dump(agent_q_train, f)\n",
    "with open(save_dir + 'dqn_normal_q_val.p', 'wb') as f:\n",
    "    pickle.dump(agent_q_val, f)\n",
    "with open(save_dir + 'dqn_normal_q_test.p', 'wb') as f:\n",
    "    pickle.dump(agent_q_test, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
